一、数据获取方式
1.企业生产的用户数据
2.数据管理咨询公司
3.政府/机构提供的公开数据
4.第三方数据平台购买数据   通过交易平台来购买各行各业的数据
5.爬虫爬取数据

二、什么是爬虫？
爬虫：抓取网页数据的程序

三、爬虫如何爬
网页三大特征
1.每个网页都有自己的URL(统一资源定位符)来进行定位
2.网页都使用HTML(超文本标记语言)
3.网页使用HTTP/HTTPS(超文本传输协议)协议来传输HTML数据

爬虫的设计思路
1.首先确定需要爬去的网页URL地址
2.通过HTTP/HTTPs协议来获取对用的HTML页面
3.提取HTML页面里有用的数据
   a.如果是需要的数据，就保持下来
   b.如果是页面里的其他的URL，那就继续执行第二步

四、为什么选择python做爬虫
  php、 java、 c/c++、 python

五、课程介绍
1.Python的基本语法知识
2.如何抓取Html页面
  http请求的处理，urllib、urllib2、requests
  处理后的请求可以模拟浏览器发送请求，获取服务器相应的文件
3.解析服务器相应的内容
  re、xpath、bs4、jsonpath、pyquery等
  使用某种描述性语言给来我们需要提取的数据定义一个匹配规则
4.如何采用动态html和验证码的处理
  通用的动态页面采集：selenium、phantomjs(无界面浏览器)：模拟真实浏览器加载js、ajax非动态
  tesseract：机器识别系统，处理简单的验证码，复杂的验证法可以手动输入，有专门的的打码平台
5.Scrapy框架(Scrapy/pyspider)
  高定制性、高性能(异步网络框架twisted)，所以下载速度很快，提供了数据存储、数据下载、提取规则等组件
6.分布式策略
  scrapy-redis,在scrapy的基础上添加了一套以Redis数据库为核心的一套组件，让Scrapy支持分布式的功能，主要在Redis里面做请求指纹去重、请求分配、数据临时存储
7.爬虫、反爬虫、反反爬虫
  User Agent、代理、验证码、动态数据加载、数据加密


六、通用爬虫  聚焦爬虫
  1.通用爬虫:搜索引擎用的爬虫
    1.目标：就是尽可能把互联网上所有的网页下载下来，放到本地服务器形成备份，在对这些网页做相关处理

    2.抓取流程：
     a、首先选取一部分已有的url，把这部分url放到带爬取队列中。
     b、从队列中取出这些url，然后解析DNS得到主机ip，然后去这个ip对应的服务器里下载html页面，保存到搜索引擎的本地服务器中，之后把
     这个爬过的url存放到已爬取队列
     c、分析这些网页内容，找出网页里其他的url连接，继续执行第二步

    3.搜索引擎如何获取一个新网站的url？
     a、主动向搜索引擎提交网址：
     b、在其他网站设置网站的外链
     c、搜索引擎和DNS服务商合作，可以快速收录新的网站

    4.通用爬虫并不是万物皆可爬，需要遵守规则：
     Robots协议：协议会写明通用爬虫可以爬取网页的权限
     Robots.txt 只是一个建议，并不是所有的爬虫都需要遵守，一般只有大型的通用爬虫需要遵守

    5.通用爬虫工作流程：爬取网页、存储数据、内容处理、提供检索、排名服务

    6.搜索引擎排名：
      1.PageRank值：根据网站的流量统计
      2.竞价排名

    7.通用爬虫的缺点：
      1.只是提供和文本相关的内容(HTML、word、pdf)等等，但是不能提供多媒体内容，比如音乐、图片或是程序、脚本等等
      2.提供的结果千篇一律，不能提供不同人不同的搜索结果
      3.不能理解人类语义上的搜索

  2.聚焦爬虫：程序员写的针对某种内容的爬虫
  面向主题爬虫，面向需求爬虫，会针对某种特定内容去爬取内容，会保证内容和需求相一致

HTTP和HTTPS
HTTP协议是一种发布和接受HTML页面的方法
HTTPS简单讲是HTTP的安全版，在HTTP下加入SSL层
SSL(安全套接层)主要用于web的安全传输协议，在传输层对网络进行加密，保障在Internet上数据传输的安全
HTTP的端口号是80
HTTPS的端口号为443

Cookie：通过在客户端记录的信息确定用户的身份
Session：通过在服务器端记录的信息确定用户的身份

######################################################################
urllib2在python3中被改为urllib.request

这个是默认的请求者，对于后台来说，这明显就是爬虫程序
urllib2默认的User Agent: Python/2.7

User Agent：发送请求带上这个，这是爬虫和反爬虫的第一步

response是服务器响应的类文件，出了支持文件操作的方法外，还支持getcode()/geturl()/info()

urllib的urlencode()接收的参数是一个字典
wd = {'  ':'   '}
urllib.urllencode(wd)
在url中要进行一个转码，汉字符号全部转成单字符
编码工作使用urllib的urlencode()函数，帮我们将key：value这样的键值对转换成'key=value'这样的字符串，解码工作可以使用urllib的unquote()函数
这也是urllib和urllib2经常在一起使用的原因

# 静态页面的爬取案例
# 贴吧案例
 pn = (page - 1) * 50

##############################################################
Get和Post请求的区别：
Get:   请求的url会附带查询参数
Post： 请求的url不带参数

对于Get请求：查询参数在QueryString里面保存
对于Post请求：查询参数在Form表单里面保存

# 有道词典
POST /translate_o?smartresult=dict&smartresult=rule HTTP/1.1
报头：
Host: fanyi.youdao.com
Connection: keep-alive
Accept: application/json, text/javascript, */*; q=0.01
X-Requested-With: XMLHttpRequest
User-Agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36
Content-Type: application/x-www-form-urlencoded; charset=UTF-8
Accept-Language: zh-CN,zh;q=0.8

url：
http://fanyi.youdao.com/translate_o?smartresult=dict&smartresult=rule

Formdata：
i=%E4%BD%A0%E5%A5%BD%0A%0A
from=AUTO
to=AUTO
smartresult=dict
client=fanyideskweb
salt=1513313106432
sign=9e722f677d6576af8469b1b6abf8727b
doctype=json
version=2.1
keyfrom=fanyi.web
action=FY_BY_REALTIME
typoResult=false

对上述报头和foemdata要进行格式处理：
正则表达式为   ^(.*):\s(.*)$    替换为   '\1':'\2'
             ^(.*)=(.*)$              '\1':'\2',

Ajax加载方式的数据获取   动态
做爬虫最需要关注的不是页面信息，而是页面信息的数据来源
Ajax 方式加载的页面，数据来源一定是json，也就是说ajax动态加载的页面后面是json1的数据存储
拿到json，就是拿到了网页的数据

它的前端是html页面，但是一定是有数据往html上传的，所以只要找到数据原就可以，不一定要在html页面上爬数据的

Ajax本身也有get和post之分，这个案例使用post的方式，，get的话就很简单了，直接取json数据
1.在https://movie.douban.com/tag/#/?sort=T&range=0,10&tags= 这个页面上获取数据，是Ajax动态获取，是使用Post方式，传了start=0，limit=100这两个参数
2.在https://movie.douban.com/j/new_search_subjects?sort=T&range=0,10&tags=&start=0&limit=100在这个页面上就是用get直接获取，因为这个页面本身就是
json了，相当于直接访问下载了，上面那个方法，有一个传参到AJax上然后找后台数据再下载的过程

####################################################
利用cookie模拟登陆

http请求处理和https请求
http是没有安全证书的，但是https是有安全证书的，但是http请求协议加上"Upgrade-Insecure-Requests" : "1"这句话之后，可以自动转https，问题不大
CA认证证书
网站域名可以变，但是CA认证证书是不可能变的，检查身份的合法性
