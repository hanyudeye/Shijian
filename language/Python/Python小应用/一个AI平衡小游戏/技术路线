https://zhuanlan.zhihu.com/p/50388723

使用增强学习，玩一个保持杆子平衡的小游戏。环境为标准的 OPenAI Gym,只使用 numpy 来创建 agent

Cart pole

增强学习：教 agent 执行某些任务/动作，但明确告诉它怎么做，Agent的目标就是在其生命周期内得到最多的奖励，
而且我们会根据是否要完成的任务相符来决定奖励的类型
增强学习 agent 的一个著名的例子是 Alphago，其中agent已经学会了如何玩围棋以最大化其奖励。

状态：
是目前游戏的样子。4个数字组成：底部小车的位置，小车的速度，杆的位置(以角度表示)和杆的角速度。这4个数字都是给定的数组

策略：
是一种函数，其输入是游戏的状态，输出agment应该在该位置采用的动作。在agent采取我们的选择后，游戏将使用下一个状态进行更新，我们会再
次将其纳入策略已做出决策。策略代表了agent背后的决策能力。

点积
状态 乘以 策略

创建我们的策略
希望agent学习策略赢得比赛或获得最大奖励
对于我们今天要开发的agent，我们将策略表示为4个数字的数组，分别代表状态的各个部分的‘重要性’然后我们会计算状态和策略数组的点积，
得到一个数字。根据数字是正数还是负数，我们将向左或向右推动小车。

假设小车在游戏中居中并且静止，杆子向右倾斜且可能倒向右边
cart_position
cart_velocity
pole_angle
pole_velocity

上述出现的四组值，和我们训练产生的一组策略相乘，如果最后的结果是整数，我们将车推向右边，否则推向左边

# 创建基础架构

# 接下来我们定一个名为play的函数，为该函数提供一个环境和一个策略数组，在环境中计算策略数组并返回分数，以及每个时步的游戏快照
我们将使用分数来判断策略的效果以及查看每个时步游戏快照来判断策略的表现。

# 有了能玩游戏的函数，并且能告诉我们的策略有多好，那么下面就创建一些策略，看看它们的效果怎样

如果我们尝试随机策略呢？
根据策略和上面创建的环境，可以用哪个它们来玩游戏，获得一个分数

只有一个数字是没有特别大的意义的，如果能够查看到agent是如何游戏的就好了

# 查看我们的agent
要查看我们的agent，我们会使用Falsk设置一个轻量级服务器，以便我们可以再浏览器中查看代理的性能。flask是一个轻量级的python
http 服务器框架，可以为我们的html ui和数据伺服。

创建一个用于渲染UI的index.html

# 搜索策略
在我们的第一句游戏中我们只是随机了一个策略，但是如果我们选择了一批策略，并且只保留那个表现最好的策略呢？

我们回到发布策略的部分，这次不是仅生成一个，而是编写一个循环来生成多个策略，并跟踪每个策略的执行情况，最终仅保存最佳策略









